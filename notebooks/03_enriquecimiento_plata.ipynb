{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0303ce6d",
   "metadata": {},
   "source": [
    "# Enriquecimiento de la capa Plata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be488a-6bf7-4136-929d-7362ff0dca6c",
   "metadata": {},
   "source": [
    "En esta notebook trabajaremos sobre los datos horarios previamente generados y la capa Plata intermedia con el objetivo de **enriquecer y completar la información** antes de su uso para análisis avanzados (clustering, PCA, reglas de asociación, etc.).\n",
    "\n",
    "### Las principales tareas realizadas son:\n",
    "- Detección de fechas y horas con datos faltantes.\n",
    "- Imputación de valores `NaN` basados en el promedio del mismo horario del día anterior y posterior.\n",
    "- Comparación entre el dataset original y el imputado.\n",
    "- Exportación del dataset horario imputado.\n",
    "- Verificación final de la cobertura completa de fechas por estación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c87778",
   "metadata": {},
   "source": [
    "## Importar las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "573fb540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importación de librerías completada.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Deshabilitar warnings futuros\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Ajustar el ancho máximo para impresión en consola\n",
    "pd.set_option('display.max_columns', None)  # Mostrar todas las columnas\n",
    "pd.set_option('display.width', 300)         # Ajustar a un ancho suficiente en consola\n",
    "pd.set_option('display.max_colwidth', None) # Evitar recortes en contenido de celdas\n",
    "\n",
    "print(\"Importación de librerías completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47201f0b",
   "metadata": {},
   "source": [
    "## Configuración de paths y carpetas del proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "069e436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciación de carpetas del proyecto completada.\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path('..').resolve()\n",
    "RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
    "BRONCE_DIR = BASE_DIR / 'data' / 'bronce'\n",
    "PLATA_DIR = Path(\"../data/plata\")\n",
    "\n",
    "archivo_plata = PLATA_DIR / \"dataset_plata_inicial.csv\"\n",
    "archivo_horario = PLATA_DIR / \"horario_archivo.csv\"\n",
    "\n",
    "print(\"Iniciación de carpetas del proyecto completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cba45b",
   "metadata": {},
   "source": [
    "## Carga del dataset y verificación de estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3fd630d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset diario cargado correctamente\n",
      "Dataset horario cargado correctamente\n",
      "\n",
      " Dataset diario:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 22 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   ESTACION              3 non-null      object        \n",
      " 1   FECHA                 3 non-null      datetime64[ns]\n",
      " 2   TEMP_MEAN             3 non-null      float64       \n",
      " 3   TEMP_MIN              3 non-null      float64       \n",
      " 4   TEMP_MAX              3 non-null      float64       \n",
      " 5   PNM_MEAN              3 non-null      float64       \n",
      " 6   PNM_MIN               3 non-null      float64       \n",
      " 7   PNM_MAX               3 non-null      float64       \n",
      " 8   HUM_MEAN              3 non-null      float64       \n",
      " 9   HUM_MIN               3 non-null      int64         \n",
      " 10  HUM_MAX               3 non-null      int64         \n",
      " 11  WIND_DIR_MEAN         3 non-null      float64       \n",
      " 12  WIND_DIR_MIN          3 non-null      int64         \n",
      " 13  WIND_DIR_MAX          3 non-null      int64         \n",
      " 14  WIND_SPEED_MEAN       3 non-null      float64       \n",
      " 15  WIND_SPEED_MIN        3 non-null      int64         \n",
      " 16  WIND_SPEED_MAX        3 non-null      int64         \n",
      " 17  TEMP_MEAN_NORM        3 non-null      float64       \n",
      " 18  PNM_MEAN_NORM         3 non-null      float64       \n",
      " 19  HUM_MEAN_NORM         3 non-null      float64       \n",
      " 20  WIND_DIR_MEAN_NORM    3 non-null      float64       \n",
      " 21  WIND_SPEED_MEAN_NORM  3 non-null      float64       \n",
      "dtypes: datetime64[ns](1), float64(14), int64(6), object(1)\n",
      "memory usage: 656.0+ bytes\n",
      "\n",
      "\n",
      "            ESTACION      FECHA  TEMP_MEAN  TEMP_MIN  TEMP_MAX  PNM_MEAN  PNM_MIN  PNM_MAX  HUM_MEAN  HUM_MIN  HUM_MAX  WIND_DIR_MEAN  WIND_DIR_MIN  WIND_DIR_MAX  WIND_SPEED_MEAN  WIND_SPEED_MIN  WIND_SPEED_MAX  TEMP_MEAN_NORM  PNM_MEAN_NORM  HUM_MEAN_NORM  WIND_DIR_MEAN_NORM  WIND_SPEED_MEAN_NORM\n",
      "0     CONCORDIA AERO 2024-06-01       17.5      14.6      22.4    1015.0   1013.3   1016.5      82.8       66       93           50.4            20           360             12.0               4              20        0.071429           1.00       1.000000            0.000000                   1.0\n",
      "1  GUALEGUAYCHU AERO 2024-06-01       18.8      14.0      24.7    1013.3   1011.6   1014.9      76.9       56       94          183.8            20           360             10.5               4              24        1.000000           0.15       0.000000            0.775131                   0.7\n",
      "2        PARANA AERO 2024-06-01       17.4      13.2      23.7    1013.0   1011.2   1014.8      80.5       57       97          222.5             0           360              7.0               0              17        0.000000           0.00       0.610169            1.000000                   0.0\n",
      "\n",
      " Dataset horario:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64 entries, 0 to 63\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   FECHA             64 non-null     object        \n",
      " 1   HORA              64 non-null     int64         \n",
      " 2   TEMP              64 non-null     float64       \n",
      " 3   HUM               64 non-null     int64         \n",
      " 4   PNM               64 non-null     float64       \n",
      " 5   DD                64 non-null     int64         \n",
      " 6   FF                64 non-null     int64         \n",
      " 7   NOMBRE            64 non-null     object        \n",
      " 8   estacion_archivo  64 non-null     object        \n",
      " 9   FECHA_HORA        64 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(2), int64(4), object(3)\n",
      "memory usage: 5.1+ KB\n",
      "\n",
      "\n",
      "        FECHA  HORA  TEMP  HUM     PNM  DD  FF          NOMBRE         estacion_archivo          FECHA_HORA\n",
      "0  2024-06-01     0  16.0   81  1016.5  50  20  CONCORDIA AERO  20240601_concordia_aero 2024-06-01 00:00:00\n",
      "1  2024-06-01     1  15.2   87  1016.2  30  17  CONCORDIA AERO  20240601_concordia_aero 2024-06-01 01:00:00\n",
      "2  2024-06-01     2  15.2   87  1016.2  30  17  CONCORDIA AERO  20240601_concordia_aero 2024-06-01 02:00:00\n",
      "3  2024-06-01     3  15.0   87  1015.6  20  17  CONCORDIA AERO  20240601_concordia_aero 2024-06-01 03:00:00\n",
      "4  2024-06-01     4  15.0   89  1015.4  30  15  CONCORDIA AERO  20240601_concordia_aero 2024-06-01 04:00:00\n"
     ]
    }
   ],
   "source": [
    "# Cargar el dataset diario\n",
    "try:\n",
    "    df_plata = pd.read_csv(archivo_plata, parse_dates=[\"FECHA\"])\n",
    "    print(\"Dataset diario cargado correctamente\")\n",
    "except FileNotFoundError:\n",
    "    print(\"El archivo diario no fue encontrado\")\n",
    "\n",
    "# Cargar el dataset horario\n",
    "try:\n",
    "    df_horario = pd.read_csv(archivo_horario, parse_dates=[\"FECHA_HORA\"])\n",
    "    print(\"Dataset horario cargado correctamente\")\n",
    "except FileNotFoundError:\n",
    "    print(\"El archivo horario no fue encontrado\")\n",
    "\n",
    "# Vista preliminar\n",
    "print(\"\\n Dataset diario:\")\n",
    "df_plata.info()\n",
    "print(\"\\n\")\n",
    "print(df_plata.head())\n",
    "\n",
    "print(\"\\n Dataset horario:\")\n",
    "df_horario.info()\n",
    "print(\"\\n\")\n",
    "print(df_horario.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2209b877-0cff-4dc7-ba04-d9ed38525d7a",
   "metadata": {},
   "source": [
    "Este paso permite validar la estructura general, tipos de datos y posibles columnas faltantes tanto en el dataset diario como en el horario. Si todo está correcto, avanzaremos con el enriquecimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a9de26",
   "metadata": {},
   "source": [
    "## Detección y análisis de fechas faltantes\n",
    "\n",
    "Una vez verificada la estructura del dataset diario, procedemos a identificar si existen fechas faltantes en la serie por estación. \n",
    "\n",
    "Esto nos permitirá decidir estrategias para tratar los días sin registros, como imputación o exclusión.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392108e6-7ec0-42ce-a42f-5f1c440573fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontraron fechas faltantes\n",
      "Empty DataFrame\n",
      "Columns: [ESTACION, FECHA]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Generar el rango completo de fechas esperadas\n",
    "fechas_totales = pd.date_range(start=df_plata['FECHA'].min(), end=df_plata['FECHA'].max(), freq='D')\n",
    "\n",
    "# Obtener todas las combinaciones posibles de fecha y estación\n",
    "estaciones = df_plata['ESTACION'].unique()\n",
    "index_completo = pd.MultiIndex.from_product([fechas_totales, estaciones], names=['FECHA', 'ESTACION'])\n",
    "\n",
    "# Reindexar para insertar NaNs explícitos en las fechas faltantes\n",
    "df_plata = df_plata.set_index(['FECHA', 'ESTACION']).reindex(index_completo).reset_index()\n",
    "\n",
    "# Verificar fechas faltantes (para exportar listado)\n",
    "faltantes = df_plata[df_plata.isnull().any(axis=1)][['ESTACION', 'FECHA']]\n",
    "\n",
    "if not faltantes.empty:\n",
    "    faltantes.to_csv(PLATA_DIR / \"fechas_faltantes.txt\", index=False, sep='\\t')\n",
    "    print(\"Fechas faltantes exportadas a:\", PLATA_DIR / \"fechas_faltantes.txt\")\n",
    "else:\n",
    "    print(\"No se encontraron fechas faltantes\")\n",
    "\n",
    "# Mostrar ejemplo si hay faltantes\n",
    "print(faltantes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b85704-5591-470e-a46e-a43346980116",
   "metadata": {},
   "source": [
    "Esta estrategia asegura que cada estación tenga una fila para cada fecha del rango, incluso si originalmente no había registros ese día. Esto deja los valores faltantes como `NaN`, que luego se tratarán."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c913c6-b13f-416a-8d06-ce3e48fc53dc",
   "metadata": {},
   "source": [
    "## Tratamiento de valores nulos\n",
    "\n",
    "Luego de verificar fechas faltantes, analizamos los valores `NaN` dentro del dataset actual para decidir estrategias de imputación o tratamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204f65cc-3a3a-4170-908f-3902830d2b18",
   "metadata": {},
   "source": [
    "### Tratamiento de datos faltantes en el dataset diario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4bfbac3-51b4-4b6b-8ceb-920aead48862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valores nulos por columna:\n",
      "FECHA                   0\n",
      "ESTACION                0\n",
      "TEMP_MEAN               0\n",
      "TEMP_MIN                0\n",
      "TEMP_MAX                0\n",
      "PNM_MEAN                0\n",
      "PNM_MIN                 0\n",
      "PNM_MAX                 0\n",
      "HUM_MEAN                0\n",
      "HUM_MIN                 0\n",
      "HUM_MAX                 0\n",
      "WIND_DIR_MEAN           0\n",
      "WIND_DIR_MIN            0\n",
      "WIND_DIR_MAX            0\n",
      "WIND_SPEED_MEAN         0\n",
      "WIND_SPEED_MIN          0\n",
      "WIND_SPEED_MAX          0\n",
      "TEMP_MEAN_NORM          0\n",
      "PNM_MEAN_NORM           0\n",
      "HUM_MEAN_NORM           0\n",
      "WIND_DIR_MEAN_NORM      0\n",
      "WIND_SPEED_MEAN_NORM    0\n",
      "dtype: int64\n",
      "\n",
      "Porcentaje de valores nulos:\n",
      "FECHA                   0.0\n",
      "ESTACION                0.0\n",
      "TEMP_MEAN               0.0\n",
      "TEMP_MIN                0.0\n",
      "TEMP_MAX                0.0\n",
      "PNM_MEAN                0.0\n",
      "PNM_MIN                 0.0\n",
      "PNM_MAX                 0.0\n",
      "HUM_MEAN                0.0\n",
      "HUM_MIN                 0.0\n",
      "HUM_MAX                 0.0\n",
      "WIND_DIR_MEAN           0.0\n",
      "WIND_DIR_MIN            0.0\n",
      "WIND_DIR_MAX            0.0\n",
      "WIND_SPEED_MEAN         0.0\n",
      "WIND_SPEED_MIN          0.0\n",
      "WIND_SPEED_MAX          0.0\n",
      "TEMP_MEAN_NORM          0.0\n",
      "PNM_MEAN_NORM           0.0\n",
      "HUM_MEAN_NORM           0.0\n",
      "WIND_DIR_MEAN_NORM      0.0\n",
      "WIND_SPEED_MEAN_NORM    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Visualizar cantidad de nulos por columna\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(df_plata.isnull().sum())\n",
    "\n",
    "# Calcular porcentaje de nulos por columna\n",
    "porcentaje_nulos = df_plata.isnull().mean() * 100\n",
    "print(\"\\nPorcentaje de valores nulos:\")\n",
    "print(porcentaje_nulos.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8a240-a6f6-4609-b13c-0ea3aea43a1c",
   "metadata": {},
   "source": [
    "Una vez identificadas las columnas afectadas, proponemos distintas estrategias para completar los datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e8c214-e0d0-4a2a-b7f6-6905872c9356",
   "metadata": {},
   "source": [
    "### Relleno con forward fill por estación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b407236-2e12-4451-9c17-492b77e80fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplo de datos tras forward fill:\n",
      "       FECHA           ESTACION  TEMP_MEAN  TEMP_MIN  TEMP_MAX  PNM_MEAN  PNM_MIN  PNM_MAX  HUM_MEAN  HUM_MIN  HUM_MAX  WIND_DIR_MEAN  WIND_DIR_MIN  WIND_DIR_MAX  WIND_SPEED_MEAN  WIND_SPEED_MIN  WIND_SPEED_MAX  TEMP_MEAN_NORM  PNM_MEAN_NORM  HUM_MEAN_NORM  WIND_DIR_MEAN_NORM  WIND_SPEED_MEAN_NORM\n",
      "0 2024-06-01     CONCORDIA AERO       17.5      14.6      22.4    1015.0   1013.3   1016.5      82.8       66       93           50.4            20           360             12.0               4              20        0.071429           1.00       1.000000            0.000000                   1.0\n",
      "1 2024-06-01  GUALEGUAYCHU AERO       18.8      14.0      24.7    1013.3   1011.6   1014.9      76.9       56       94          183.8            20           360             10.5               4              24        1.000000           0.15       0.000000            0.775131                   0.7\n",
      "2 2024-06-01        PARANA AERO       17.4      13.2      23.7    1013.0   1011.2   1014.8      80.5       57       97          222.5             0           360              7.0               0              17        0.000000           0.00       0.610169            1.000000                   0.0\n"
     ]
    }
   ],
   "source": [
    "# Ordenar por estación y fecha para aplicar forward fill correctamente\n",
    "df_plata_ffill = df_plata.sort_values(['ESTACION', 'FECHA']).copy()\n",
    "df_plata_ffill.update(df_plata.groupby('ESTACION').ffill())\n",
    "\n",
    "# Vista previa de ejemplo tras forward fill\n",
    "print(\"\\nEjemplo de datos tras forward fill:\")\n",
    "print(df_plata_ffill.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacaf95f-a649-4971-b4ee-9c042fe1db11",
   "metadata": {},
   "source": [
    "### Imputación con la media de cada estación (solo para columnas numéricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6159b92c-f4f2-4554-8400-2d043628374f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valores nulos después de imputación con medias:\n",
      "TEMP_MEAN          0\n",
      "PNM_MEAN           0\n",
      "HUM_MEAN           0\n",
      "WIND_SPEED_MEAN    0\n",
      "WIND_DIR_MEAN      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Imputar con la media por estación\n",
    "columnas_a_imputar = ['TEMP_MEAN', 'PNM_MEAN', 'HUM_MEAN', 'WIND_SPEED_MEAN', 'WIND_DIR_MEAN']\n",
    "\n",
    "for col in columnas_a_imputar:\n",
    "    df_plata_ffill[col] = df_plata_ffill.groupby('ESTACION')[col].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# Verificar resultado tras imputación\n",
    "print(\"\\nValores nulos después de imputación con medias:\")\n",
    "print(df_plata_ffill[columnas_a_imputar].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3908a838-a9c0-4c8f-b1f5-b6005cfa8706",
   "metadata": {},
   "source": [
    "Estas estrategias permiten garantizar que las variables derivadas a construir se basen en datos consistentes, sin afectar la distribución ni introducir sesgos evidentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e25e56-6de6-47c7-9d84-967dd5acbdd0",
   "metadata": {},
   "source": [
    "### Tratamiento de datos faltantes en el dataset horario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dbd7603-b5fb-446f-9f57-f12b36817782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diferencia de tamaño (horas originales vs completadas por horario habitual):\n",
      "Original: 64\n",
      "Completo: 64\n",
      "\n",
      "Ejemplo de datos horarios con NaN insertados:\n",
      "Empty DataFrame\n",
      "Columns: [NOMBRE, FECHA_HORA, FECHA, HORA, TEMP, HUM, PNM, DD, FF, estacion_archivo]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Detectar horarios reales de cada estación\n",
    "df_horario['HORA'] = df_horario['FECHA_HORA'].dt.hour\n",
    "horarios_por_estacion = df_horario.groupby('NOMBRE')['HORA'].value_counts().unstack(fill_value=0)\n",
    "horarios_mas_frecuentes = horarios_por_estacion.idxmax(axis=1)\n",
    "\n",
    "# Detectar horarios outlier (menos del 5% de los días)\n",
    "outliers_horarios = {}\n",
    "for estacion in horarios_por_estacion.index:\n",
    "    total_dias = df_horario[df_horario['NOMBRE'] == estacion]['FECHA_HORA'].dt.date.nunique()\n",
    "    outliers = horarios_por_estacion.loc[estacion][\n",
    "        horarios_por_estacion.loc[estacion] / total_dias < 0.05\n",
    "    ].index.tolist()\n",
    "    if outliers:\n",
    "        outliers_horarios[estacion] = outliers\n",
    "\n",
    "# Crear index completo por estación y sus horarios típicos\n",
    "df_horario['FECHA'] = df_horario['FECHA_HORA'].dt.floor('D')\n",
    "estaciones_h = df_horario['NOMBRE'].unique()\n",
    "fecha_h_min = df_horario['FECHA'].min()\n",
    "fecha_h_max = df_horario['FECHA'].max()\n",
    "rango_fechas = pd.date_range(start=fecha_h_min, end=fecha_h_max, freq='D')\n",
    "\n",
    "# Crear combinaciones válidas por estación\n",
    "porcentaje_frecuencia = 0.05 # al menos en 5% de los días\n",
    "\n",
    "index_completo_personalizado = []\n",
    "for estacion in estaciones_h:\n",
    "    total_dias_estacion = df_horario[df_horario['NOMBRE'] == estacion]['FECHA'].nunique()\n",
    "    horas_validas = horarios_por_estacion.columns[\n",
    "        (horarios_por_estacion.loc[estacion] / total_dias_estacion) >= porcentaje_frecuencia  \n",
    "    ].tolist()\n",
    "\n",
    "    for fecha in rango_fechas:\n",
    "        for hora in horas_validas:\n",
    "            index_completo_personalizado.append((estacion, pd.Timestamp(fecha + pd.Timedelta(hours=hora))))\n",
    "\n",
    "index_completo_h = pd.MultiIndex.from_tuples(index_completo_personalizado, names=['NOMBRE', 'FECHA_HORA'])\n",
    "\n",
    "# Reindexar para insertar valores faltantes en los horarios esperados únicamente\n",
    "df_horario_completo = df_horario.set_index(['NOMBRE', 'FECHA_HORA']).reindex(index_completo_h).reset_index()\n",
    "\n",
    "# Verificación\n",
    "print(\"\\nDiferencia de tamaño (horas originales vs completadas por horario habitual):\")\n",
    "print(\"Original:\", len(df_horario))\n",
    "print(\"Completo:\", len(df_horario_completo))\n",
    "print(\"\\nEjemplo de datos horarios con NaN insertados:\")\n",
    "print(df_horario_completo[df_horario_completo.isnull().any(axis=1)].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbe64ec-63a1-4df4-99b5-7b3a8f844f5f",
   "metadata": {},
   "source": [
    "### Mostrar horarios outliers detectados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3efa97cf-8e8b-4e6d-bf2c-5f77410e2855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Registros reales en horarios atípicos:\n",
      " - GUALEGUAYCHU AERO: [0, 1, 2, 3, 4, 5, 22, 23]\n"
     ]
    }
   ],
   "source": [
    "# Visualizar registros reales en horarios atípicos detectados\n",
    "print(\"\\n Registros reales en horarios atípicos:\")\n",
    "for estacion, horas in outliers_horarios.items():\n",
    "    print(f\" - {estacion}: {horas}\")\n",
    "\n",
    "# Registrar los registros reales que ocurren en horarios atípicos\n",
    "df_outliers_registros = []\n",
    "for estacion, horas_outlier in outliers_horarios.items():\n",
    "    registros_outlier = df_horario[\n",
    "        (df_horario['NOMBRE'] == estacion) &\n",
    "        (df_horario['HORA'].isin(horas_outlier))\n",
    "    ]\n",
    "    if not registros_outlier.empty:\n",
    "        df_outliers_registros.append(registros_outlier)\n",
    "\n",
    "# Concatenar y exportar si hay registros\n",
    "if df_outliers_registros:\n",
    "    df_outliers_concat = pd.concat(df_outliers_registros)\n",
    "    archivo_outliers = PLATA_DIR / \"registros_horarios_atipicos.csv\"\n",
    "    df_outliers_concat.to_csv(archivo_outliers, index=False)\n",
    "    print(\"\\n Archivo exportado con registros reales en horarios atípicos:\")\n",
    "    print(archivo_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555ab9e1-38b0-422b-8ac8-fabee6b2cc05",
   "metadata": {},
   "source": [
    "## Exportar datasets intermedios (antes de procesar los NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e123bf3d-a7ba-4ed6-b638-889d892267bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos generados correctamente\n"
     ]
    }
   ],
   "source": [
    "# Exportar datasets intermedios (si se desea conservar)\n",
    "df_plata.to_csv(PLATA_DIR / \"dataset_intermedio_horario_con_nan.csv\", index=False)\n",
    "df_plata_ffill.to_csv(PLATA_DIR / \"dataset_intermedio_horario_ffill.csv\", index=False)\n",
    "df_horario_completo.to_csv(PLATA_DIR / \"dataset_intermedio_horario_completo.csv\", index=False)\n",
    "\n",
    "print(\"Archivos generados correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "387836d4-c0de-457a-b4ff-7db2548ab541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            NOMBRE          FECHA_HORA      FECHA  HORA  TEMP  HUM     PNM   DD  FF         estacion_archivo\n",
      "0   CONCORDIA AERO 2024-06-01 00:00:00 2024-06-01     0  16.0   81  1016.5   50  20  20240601_concordia_aero\n",
      "1   CONCORDIA AERO 2024-06-01 01:00:00 2024-06-01     1  15.2   87  1016.2   30  17  20240601_concordia_aero\n",
      "2   CONCORDIA AERO 2024-06-01 02:00:00 2024-06-01     2  15.2   87  1016.2   30  17  20240601_concordia_aero\n",
      "3   CONCORDIA AERO 2024-06-01 03:00:00 2024-06-01     3  15.0   87  1015.6   20  17  20240601_concordia_aero\n",
      "4   CONCORDIA AERO 2024-06-01 04:00:00 2024-06-01     4  15.0   89  1015.4   30  15  20240601_concordia_aero\n",
      "..             ...                 ...        ...   ...   ...  ...     ...  ...  ..                      ...\n",
      "59     PARANA AERO 2024-06-01 19:00:00 2024-06-01    19  19.7   70  1011.8   90   6     20240601_parana_aero\n",
      "60     PARANA AERO 2024-06-01 20:00:00 2024-06-01    20  18.0   82  1012.0  360   4     20240601_parana_aero\n",
      "61     PARANA AERO 2024-06-01 21:00:00 2024-06-01    21  17.5   86  1012.4  330   6     20240601_parana_aero\n",
      "62     PARANA AERO 2024-06-01 22:00:00 2024-06-01    22  16.5   90  1012.5  360   4     20240601_parana_aero\n",
      "63     PARANA AERO 2024-06-01 23:00:00 2024-06-01    23  15.1   93  1012.4  120   4     20240601_parana_aero\n",
      "\n",
      "[64 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_horario_completo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b914b3-5333-417a-b3d7-4c406e1ee6ff",
   "metadata": {},
   "source": [
    "## Imputación de datos faltantes basada en promedio entre días anterior y posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92c55f63-780e-4528-9572-12334333c157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo exportado: ../data/plata/dataset_plata_horario_final.csv\n"
     ]
    }
   ],
   "source": [
    "# Variables a imputar\n",
    "variables_objetivo = ['TEMP', 'HUM', 'PNM', 'DD', 'FF']\n",
    "\n",
    "df_interp = df_horario_completo.copy()\n",
    "\n",
    "# Asegurar FECHA y HORA correctas\n",
    "df_interp['FECHA'] = df_interp['FECHA_HORA'].dt.date\n",
    "df_interp['HORA'] = df_interp['FECHA_HORA'].dt.hour\n",
    "\n",
    "# Ordenar por estación, fecha y hora\n",
    "df_interp = df_interp.sort_values(by=['NOMBRE', 'FECHA', 'HORA'])\n",
    "\n",
    "# Función de imputación por promedio entre día anterior y posterior\n",
    "def imputar_valores(grupo):\n",
    "    grupo = grupo.copy()  # para evitar advertencias de SettingWithCopy\n",
    "    for var in variables_objetivo:\n",
    "        for idx, fila in grupo.iterrows():\n",
    "            if pd.isna(fila[var]):\n",
    "                hora = fila['HORA']\n",
    "                fecha = fila['FECHA']\n",
    "\n",
    "                # Buscar el valor del día anterior\n",
    "                val_ant = grupo[(grupo['HORA'] == hora) & (grupo['FECHA'] < fecha)][var].last_valid_index()\n",
    "                val_ant = grupo.at[val_ant, var] if val_ant is not None else None\n",
    "\n",
    "                # Buscar el valor del día posterior\n",
    "                val_post = grupo[(grupo['HORA'] == hora) & (grupo['FECHA'] > fecha)][var].first_valid_index()\n",
    "                val_post = grupo.at[val_post, var] if val_post is not None else None\n",
    "\n",
    "                # Asignar promedio o valor disponible\n",
    "                if val_ant is not None and val_post is not None:\n",
    "                    grupo.at[idx, var] = round((val_ant + val_post) / 2, 1)\n",
    "                elif val_ant is not None:\n",
    "                    grupo.at[idx, var] = val_ant\n",
    "                elif val_post is not None:\n",
    "                    grupo.at[idx, var] = val_post\n",
    "    return grupo\n",
    "\n",
    "# Aplicar por estación SIN include_groups\n",
    "df_interp = (\n",
    "    df_interp.groupby('NOMBRE', group_keys=False)\n",
    "    .apply(imputar_valores)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Redondear valores numéricos a 1 decimal\n",
    "for var in variables_objetivo:\n",
    "    df_interp[var] = df_interp[var].round(1)\n",
    "\n",
    "# Ajustar tipos de columnas\n",
    "df_interp['HORA'] = df_interp['HORA'].astype('int64')\n",
    "if 'estacion_archivo' in df_interp.columns:\n",
    "    df_interp['estacion_archivo'] = df_interp['estacion_archivo'].astype('int64', errors='ignore')\n",
    "\n",
    "# Exportar\n",
    "archivo_imputado = PLATA_DIR / \"dataset_plata_horario_final.csv\"\n",
    "df_interp.to_csv(archivo_imputado, index=False)\n",
    "print(f\"Archivo exportado: {archivo_imputado}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601075f8-481b-4421-86ea-08dde5c8ec10",
   "metadata": {},
   "source": [
    "## Generar archivo diario a partir de la imputación de los datos faltantes en el dato_horario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f25ddba3-bd7d-4118-bc51-312318bc688b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columnas finales: ['ESTACION', 'FECHA', 'TEMP_MEAN', 'TEMP_MIN', 'TEMP_MAX', 'PNM_MEAN', 'PNM_MIN', 'PNM_MAX', 'HUM_MEAN', 'HUM_MIN', 'HUM_MAX', 'WIND_DIR_MEAN', 'WIND_DIR_MIN', 'WIND_DIR_MAX', 'WIND_SPEED_MEAN', 'WIND_SPEED_MIN', 'WIND_SPEED_MAX', 'TEMP_MEAN_NORM', 'PNM_MEAN_NORM', 'HUM_MEAN_NORM', 'WIND_DIR_MEAN_NORM', 'WIND_SPEED_MEAN_NORM']\n",
      "Filas: 3 | Columnas: 22\n",
      "ESTACION\n",
      "CONCORDIA AERO       1\n",
      "GUALEGUAYCHU AERO    1\n",
      "PARANA AERO          1\n",
      "dtype: int64\n",
      "Archivo diario imputado exportado: ../data/plata/dataset_plata_diario_final.csv\n"
     ]
    }
   ],
   "source": [
    "# Generar dataset diario imputado (todas las estaciones)\n",
    "\n",
    "# Agrupar por estación y fecha\n",
    "df_diario_imputado = df_interp.groupby(['NOMBRE', 'FECHA']).agg(\n",
    "    TEMP_MEAN=('TEMP', 'mean'),\n",
    "    TEMP_MIN=('TEMP', 'min'),\n",
    "    TEMP_MAX=('TEMP', 'max'),\n",
    "    PNM_MEAN=('PNM', 'mean'),\n",
    "    PNM_MIN=('PNM', 'min'),\n",
    "    PNM_MAX=('PNM', 'max'),\n",
    "    HUM_MEAN=('HUM', 'mean'),\n",
    "    HUM_MIN=('HUM', 'min'),\n",
    "    HUM_MAX=('HUM', 'max'),\n",
    "    WIND_DIR_MEAN=('DD', 'mean'),\n",
    "    WIND_DIR_MIN=('DD', 'min'),\n",
    "    WIND_DIR_MAX=('DD', 'max'),\n",
    "    WIND_SPEED_MEAN=('FF', 'mean'),\n",
    "    WIND_SPEED_MIN=('FF', 'min'),\n",
    "    WIND_SPEED_MAX=('FF', 'max')\n",
    ").reset_index()\n",
    "\n",
    "# Renombrar y ordenar\n",
    "df_diario_imputado.rename(columns={'NOMBRE':'ESTACION'}, inplace=True)\n",
    "df_diario_imputado['FECHA'] = pd.to_datetime(df_diario_imputado['FECHA'])\n",
    "df_diario_imputado = df_diario_imputado.sort_values(by=['ESTACION','FECHA']).reset_index(drop=True)\n",
    "\n",
    "# Ajustes de tipos y redondeo\n",
    "\n",
    "# Redondear medias a 1 decimal\n",
    "cols_float = ['TEMP_MEAN','PNM_MEAN','HUM_MEAN','WIND_DIR_MEAN','WIND_SPEED_MEAN']\n",
    "df_diario_imputado[cols_float] = df_diario_imputado[cols_float].round(1)\n",
    "\n",
    "# Convertir min y max a enteros\n",
    "cols_int = [\n",
    "    'TEMP_MIN','TEMP_MAX','PNM_MIN','PNM_MAX',\n",
    "    'HUM_MIN','HUM_MAX',\n",
    "    'WIND_DIR_MIN','WIND_DIR_MAX',\n",
    "    'WIND_SPEED_MIN','WIND_SPEED_MAX'\n",
    "]\n",
    "df_diario_imputado[cols_int] = df_diario_imputado[cols_int].round().astype(int)\n",
    "\n",
    "# Normalización Min-Max de las variables MEAN\n",
    "\n",
    "variables_mean = ['TEMP_MEAN','PNM_MEAN','HUM_MEAN','WIND_DIR_MEAN','WIND_SPEED_MEAN']\n",
    "for var in variables_mean:\n",
    "    col_norm = var + '_NORM'\n",
    "    min_val = df_diario_imputado[var].min()\n",
    "    max_val = df_diario_imputado[var].max()\n",
    "    df_diario_imputado[col_norm] = ((df_diario_imputado[var] - min_val) / (max_val - min_val)).round(5)\n",
    "\n",
    "# Validación y exportación\n",
    "\n",
    "print(\"\\nColumnas finales:\", df_diario_imputado.columns.tolist())\n",
    "print(\"Filas:\", len(df_diario_imputado), \"| Columnas:\", len(df_diario_imputado.columns))\n",
    "print(df_diario_imputado.groupby('ESTACION').size())\n",
    "\n",
    "# Guardar el dataset diario imputado completo\n",
    "archivo_diario_imputado = PLATA_DIR / \"dataset_plata_diario_final.csv\"\n",
    "df_diario_imputado.to_csv(archivo_diario_imputado, index=False)\n",
    "print(f\"Archivo diario imputado exportado: {archivo_diario_imputado}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5a1e2-540a-4687-a19e-2fbad388c299",
   "metadata": {},
   "source": [
    "## Verificar las imputaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b6171ed-7840-45f6-bebd-8b074f8307f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores restantes faltantes por variable:\n",
      "TEMP    0\n",
      "HUM     0\n",
      "PNM     0\n",
      "DD      0\n",
      "FF      0\n",
      "dtype: int64\n",
      "\n",
      "Ejemplos de filas con valores aún faltantes:\n",
      "Empty DataFrame\n",
      "Columns: [NOMBRE, FECHA_HORA, FECHA, HORA, TEMP, HUM, PNM, DD, FF, estacion_archivo]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Verificación de imputación final\n",
    "print(\"Valores restantes faltantes por variable:\")\n",
    "print(df_interp[variables_objetivo].isnull().sum())\n",
    "\n",
    "# Vista previa de algunos valores aún faltantes (si existen)\n",
    "print(\"\\nEjemplos de filas con valores aún faltantes:\")\n",
    "print(df_interp[df_interp[variables_objetivo].isnull().any(axis=1)].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582557e9-6395-4024-bf21-354896d3d44d",
   "metadata": {},
   "source": [
    "# Contar imputaciones por columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b35aff0-0b2a-4654-b75a-2da41925b40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen de imputaciones por variable:\n",
      " - TEMP: 0 valores imputados\n",
      " - HUM: 0 valores imputados\n",
      " - PNM: 0 valores imputados\n",
      " - DD: 0 valores imputados\n",
      " - FF: 0 valores imputados\n",
      "\n",
      "Ejemplos de imputaciones realizadas:\n"
     ]
    }
   ],
   "source": [
    "imputaciones = {}\n",
    "for var in variables_objetivo:\n",
    "    # Detectar índices donde original es NaN pero imputado tiene valor\n",
    "    mask_imputado = df_horario_completo[var].isna() & df_interp[var].notna()\n",
    "    imputaciones[var] = mask_imputado.sum()\n",
    "\n",
    "# Mostrar resumen\n",
    "print(\"Resumen de imputaciones por variable:\")\n",
    "for var, count in imputaciones.items():\n",
    "    print(f\" - {var}: {count} valores imputados\")\n",
    "\n",
    "# Mostrar ejemplos comparativos (solo filas donde hubo imputación)\n",
    "print(\"\\nEjemplos de imputaciones realizadas:\")\n",
    "for var in variables_objetivo:\n",
    "    mask = df_horario_completo[var].isna() & df_interp[var].notna()\n",
    "    if mask.any():\n",
    "        print(f\"\\nVariable: {var}\")\n",
    "        print(df_interp.loc[mask, ['FECHA_HORA', 'NOMBRE', var]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef2e28a-e09d-4f54-931b-233ace89e1c0",
   "metadata": {},
   "source": [
    "# Visualización de imputación de los días faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fde00ba-b535-4101-aa38-7764982715d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Estación: CONCORDIA AERO ===\n",
      "\n",
      "=== Estación: GUALEGUAYCHU AERO ===\n",
      "\n",
      "=== Estación: PARANA AERO ===\n",
      "\n",
      "Resumen de verificación de imputaciones por estación:\n",
      "            Estación  Total días faltantes  Días completos  Días con NaN\n",
      "0     CONCORDIA AERO                     0               0             0\n",
      "1  GUALEGUAYCHU AERO                     0               0             0\n",
      "2        PARANA AERO                     0               0             0\n"
     ]
    }
   ],
   "source": [
    "# Carpeta con los archivos de días faltantes\n",
    "FALTANTES_DIR = Path(\"../data/faltantes\")  # Ajustar al directorio correcto\n",
    "variables_objetivo = ['TEMP', 'HUM', 'PNM', 'DD', 'FF']\n",
    "\n",
    "# Función para leer días faltantes de un archivo\n",
    "def leer_dias_faltantes(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    fechas = [line.strip() for line in lines if line.strip() and line.strip()[0].isdigit()]\n",
    "    return pd.to_datetime(fechas).date\n",
    "\n",
    "# Recorrer todos los archivos .txt de días faltantes\n",
    "faltantes_files = list(FALTANTES_DIR.glob(\"*.txt\"))\n",
    "\n",
    "resumen_resultados = []\n",
    "\n",
    "for file_path in faltantes_files:\n",
    "    estacion = file_path.stem.replace(\"dias_faltantes_\", \"\").replace(\"_\", \" \").upper()\n",
    "    dias_faltantes = leer_dias_faltantes(file_path)\n",
    "    \n",
    "    print(f\"\\n=== Estación: {estacion} ===\")\n",
    "    resultados_estacion = {\"Estación\": estacion, \"Total días faltantes\": len(dias_faltantes), \"Días completos\": 0, \"Días con NaN\": 0}\n",
    "    \n",
    "    for fecha in dias_faltantes:\n",
    "        subset_original = df_horario_completo[(df_horario_completo['NOMBRE'].str.upper() == estacion) & (df_horario_completo['FECHA'] == fecha)]\n",
    "        subset_imputado = df_interp[(df_interp['NOMBRE'].str.upper() == estacion) & (df_interp['FECHA'] == fecha)]\n",
    "        \n",
    "        if subset_imputado[variables_objetivo].isnull().any().any():\n",
    "            resultados_estacion[\"Días con NaN\"] += 1\n",
    "            print(f\"\\nFecha {fecha} aún con NaN\")\n",
    "        else:\n",
    "            resultados_estacion[\"Días completos\"] += 1\n",
    "            print(f\"\\nFecha {fecha} imputada correctamente\")\n",
    "        \n",
    "        # Comparar antes y después\n",
    "        if not subset_imputado.empty:\n",
    "            print(\"\\n--- Antes (Original con NaN) ---\")\n",
    "            print(subset_original[['FECHA_HORA', 'NOMBRE'] + variables_objetivo])\n",
    "            print(\"\\n--- Después (Imputado) ---\")\n",
    "            print(subset_imputado[['FECHA_HORA', 'NOMBRE'] + variables_objetivo])\n",
    "    \n",
    "    resumen_resultados.append(resultados_estacion)\n",
    "\n",
    "# Mostrar resumen final\n",
    "df_resumen = pd.DataFrame(resumen_resultados)\n",
    "print(\"\\nResumen de verificación de imputaciones por estación:\")\n",
    "print(df_resumen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a775e-65d6-4257-87bd-2ca94a7ec61b",
   "metadata": {},
   "source": [
    "# Verificación de datos faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "505c7ad1-8fc5-4237-bb4f-73822d928246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontraron fechas faltantes después de la imputación.\n",
      "\n",
      "Ejemplo de fechas faltantes:\n",
      "Empty DataFrame\n",
      "Columns: [NOMBRE, FECHA]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame reducido solo con días únicos por estación\n",
    "df_fechas = df_interp[['FECHA', 'NOMBRE']].drop_duplicates()\n",
    "\n",
    "# Generar rango completo de fechas\n",
    "fechas_totales = pd.date_range(start=df_fechas['FECHA'].min(), end=df_fechas['FECHA'].max(), freq='D')\n",
    "\n",
    "# Estaciones\n",
    "estaciones = df_fechas['NOMBRE'].unique()\n",
    "# Crear todas las combinaciones posibles (fecha, estación)\n",
    "index_completo = pd.MultiIndex.from_product([fechas_totales, estaciones], names=['FECHA', 'NOMBRE'])\n",
    "\n",
    "# Reindexar\n",
    "df_check = df_fechas.set_index(['FECHA', 'NOMBRE']).reindex(index_completo).reset_index()\n",
    "\n",
    "# Verificar faltantes\n",
    "faltantes = df_check[df_check.isnull().any(axis=1)][['NOMBRE', 'FECHA']]\n",
    "\n",
    "# Exportar resultados\n",
    "if not faltantes.empty:\n",
    "    archivo_faltantes_final = PLATA_DIR / \"fechas_faltantes_post_imputacion.txt\"\n",
    "    faltantes.to_csv(archivo_faltantes_final, index=False, sep='\\t')\n",
    "    print(f\"Fechas faltantes exportadas a: {archivo_faltantes_final}\")\n",
    "else:\n",
    "    print(\"No se encontraron fechas faltantes después de la imputación.\")\n",
    "\n",
    "# Vista rápida\n",
    "print(\"\\nEjemplo de fechas faltantes:\")\n",
    "print(faltantes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fa51e0-6e02-4cda-820c-d2c6f8b5c2eb",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "\n",
    "En este notebook hemos completado el proceso de **enriquecimiento de la Capa Plata**.\n",
    "\n",
    "Se garantiza:\n",
    "- **Datos horarios completos** por estación para el período de análisis.\n",
    "- **Tratamiento adecuado de valores faltantes**, aplicando imputaciones coherentes con el comportamiento histórico de cada estación.\n",
    "- Generación de un **dataset final imputado** que sirve como base para análisis avanzados (clustering, PCA, detección de eventos).\n",
    "\n",
    "Con esta preparación, los datos están listos para las tareas de **minería de datos y categorización**, que abordaremos en la clase siguiente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
